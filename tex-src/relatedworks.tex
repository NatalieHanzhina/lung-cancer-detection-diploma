\section{Сверточные нейронные сети}

Сверточные нейронные сети являются одной из основных технологией в компьютерном зрении и глубоком обучении. Появление сверточных сетей наряду с графическими процессорами (GPU), рассчитанными на массивные параллельные вычисления, позволило совершить огромный прорыв в области решения задач классификации, локализации, сегментации и т.д.

\subsection{ResNet (Residual Neural Network, Остаточная нейронная сеть)}

ResNet -  это популярная сверточная сеть, предложенная в работе \cite{he2016deep}. Основной особенностью являются skip свзязи - переходы между слоями, не являющимися соседними к друг другу и помогающие в борьбе с проблемой затухающего градиента.

\section{Кодировщик-декодировщик (Encoder-Decoder)}

Encoder-Decoder является распространенной архитектурой нейронных сетей, подразумевающуй наличие кодировщика и декодировщика. Кодировщик - это последовательность слоев сети, которые преобразуют вход, существенно сокращая его размерность. На выходе кодировщика получается некоторый тензор из латентного пространства, который должен эффективно представлять вход сети, используя для этого данные меньше размерности. К выходу кодировщика применяется декодировщик, который в свою очередь преобразует латентное представление входа сети в некоторые данные большей размерности.

\section{Region Proposal Network (RPN)}

Впервые RPN была предложена в работе \cite{ren2015faster} как часть сети Faster-R-CNN и предназначается для решения задачи локализации. RPN надстраивается над выходом сверточной сети, содержащем feature map исходного изображения. Помимо наличия основного входа RPN параметризуется также и набором анкоров - участков изображения, построенных на основании размерностей исходного изображения. Для каждого из анкоров RPN необходимо подать на выход предсказание, насколько этот анкор совпдает с локализируемыми участками, и набор несколько преобразованных координат анкора.

\section{Squeeze and Excitation (SE)}

Модуль SE был предложен в 2017 году в работе \cite{hu2018squeeze} и показал SOTA результат в соревновании ImageNet. Данный модуль, который предполагается включать в сеть после каждого сверточного блока, позволяет использовать зависимости между различными каналами блока и масштабировать с помощью вектора коэффициентов, обучаемых в небольшой побочной сети.

\section{Генеративные противоборствующие сети}

Генеративные противоборствующие сети эффективно используются не только для генерации реалистичных фотографий, лиц и т.д., которые можно было бы использовать вместо настоящих фотографий для некоторых конечных целей, но также широко применяются для аугментации данных, которые предполагается использовать для обучения.

Основная идея GAN состоит в создании двух независимых сетей - генератора и дискриминатора, и задачей генератора является генерация реалистичных объектов по некоторому входу из латентного пространства, а задачей дискриминатора - классификация объектов на генерированные и оригинальные.

Обучение GAN представляет из себя минимаксную игру, где генератор стремится увеличить функцию потерь дискриминатора, а дискриминатор - напротив, уменьшить ее.

Также выделяют отдельный класс генеративных противоборствующих сетей - Deep Convolutional GAN (DCGAN), которые используют глубокие сверточные сети в качестве моделей генератора и дискриминатора.

\subsection{Условные генеративные противоборстующие сети (Conditional GAN, CGAN)}

В данной вариации GAN на вход генератору подаются не только данные из латентного пространства, но и некоторые данные условия (condition). Таким образом генератору приходится подстраивать генерируемые данные под это условие, чтобы они выглядели реалистично.

\subsection{Wasserstein GAN (WGAN) \cite{arjovsky2017wasserstein}}

Данная модификация GAN заключается в 


\section{Адаптивная нормализация объектов}

Адаптивная нормализация объектов - технология нормализации выходов внутренних слоев сети, которая была предложена в работе \cite{huang2017arbitrary} для совершенствования модели переноса стиля одного изображения на другое

Это афинное преобразование входа $x$ параметризованное некоторым $y$:

\begin{equation}
AdaIN(x, y) = \sigma(y)(\dfrac{x - \mu(x)}{\sigma(x)}) + \mu(y) \label{eq:adain}
\end{equation}

Авторы предполагали использовать стиль в качестве $y$, однако в последствии данную нормализацию стали применять.

В области генеративных противоборствующих сетей AdaIN можно применять не только для решения задачи переноса стиля, но и для обучения эффективных параметров нормализации. В роли $y$ может выступать выход некоторой побочной сети, принимающей на вход латентный вектор.